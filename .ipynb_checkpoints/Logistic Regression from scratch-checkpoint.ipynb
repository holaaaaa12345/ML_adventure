{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a95648d3",
   "metadata": {},
   "source": [
    "# Logistic Regression from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "113855f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185198fb",
   "metadata": {},
   "source": [
    "### The Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b114a5b6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>area_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>10.38</td>\n",
       "      <td>17.99</td>\n",
       "      <td>1001.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>17.77</td>\n",
       "      <td>20.57</td>\n",
       "      <td>1326.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>21.25</td>\n",
       "      <td>19.69</td>\n",
       "      <td>1203.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>20.38</td>\n",
       "      <td>11.42</td>\n",
       "      <td>386.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>14.34</td>\n",
       "      <td>20.29</td>\n",
       "      <td>1297.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>1</td>\n",
       "      <td>22.39</td>\n",
       "      <td>21.56</td>\n",
       "      <td>1479.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>1</td>\n",
       "      <td>28.25</td>\n",
       "      <td>20.13</td>\n",
       "      <td>1261.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>1</td>\n",
       "      <td>28.08</td>\n",
       "      <td>16.60</td>\n",
       "      <td>858.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>1</td>\n",
       "      <td>29.33</td>\n",
       "      <td>20.60</td>\n",
       "      <td>1265.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>0</td>\n",
       "      <td>24.54</td>\n",
       "      <td>7.76</td>\n",
       "      <td>181.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>569 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     diagnosis  texture_mean  radius_mean  area_mean\n",
       "0            1         10.38        17.99     1001.0\n",
       "1            1         17.77        20.57     1326.0\n",
       "2            1         21.25        19.69     1203.0\n",
       "3            1         20.38        11.42      386.1\n",
       "4            1         14.34        20.29     1297.0\n",
       "..         ...           ...          ...        ...\n",
       "564          1         22.39        21.56     1479.0\n",
       "565          1         28.25        20.13     1261.0\n",
       "566          1         28.08        16.60      858.1\n",
       "567          1         29.33        20.60     1265.0\n",
       "568          0         24.54         7.76      181.0\n",
       "\n",
       "[569 rows x 4 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"D:/jupyter notebook/Learning_dataset/bdiag.csv\")[[\"diagnosis\", \"texture_mean\", \"radius_mean\", \"area_mean\"]]\n",
    "df[\"diagnosis\"] = df[\"diagnosis\"].replace({\"M\":1, \"B\":0})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb64938",
   "metadata": {},
   "source": [
    "### Generalized Linear Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2deed8fe",
   "metadata": {},
   "source": [
    "Essentially an extension of linear model, wherein the response variable need not be normally distributed. In logistic regression, the response variable is bernoulli distributed and the assumed link function is the logit, the inverse of which is the sigmoid."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d03a23e8",
   "metadata": {},
   "source": [
    "### Maximum Likelihood Estimation for Bernoulli Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7263686",
   "metadata": {},
   "source": [
    "The cost function is called cross-entropy, more specifically log cost. It can be obtained through MLE, i.e. maximizing the likelihood function.\n",
    "\n",
    "$$L(x_{i}; p) = p^{x_{i}}(1-p)^{1-x_{i}}$$ \n",
    "\n",
    "Applying log to both sides (it can be done due to the monotonic nature of log)\n",
    "\n",
    "$$\\log(L(x_{i}; p)) = \\sum_{i=0}^{n}x_{i}\\log{p}+(1-x_{i})\\log(1-p)$$\n",
    "\n",
    "Adding the negative sign to change the purpose from maximizing to minimizing and multiplying by $\\frac{1}{n}$ to obtain the average loss.\n",
    "\n",
    "$$\\textrm{cross-entropy binary cost function}= -\\frac{1}{n}\\sum_{i=0}^{n}x_{i}\\log{p}+(1-x_{i})\\log(1-p)$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31503938",
   "metadata": {},
   "source": [
    "### Cost function for logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22413f78",
   "metadata": {},
   "source": [
    "In logistic regression, the $p$ parameter is assumed to be obtained through the sigmoid of linear combination of input features.\n",
    "\n",
    "$$\\textrm{sigmoid(x)} = \\frac{1}{1+e^{-X}}$$\n",
    "\n",
    "$$\\textrm{linear combination of input features} = X\\vec{w}$$ \n",
    "\n",
    "where X is an $(m+1)\\times n$ matrix and W is an $n\\times1$ matrix. Therefore,\n",
    "\n",
    "$$p = h_{\\theta}(X) = \\frac{1}{1+e^{-X\\vec{w}}}$$\n",
    "\n",
    "$$\\textrm{cross-entropy binary cost function}= -\\frac{1}{n}\\sum_{i=0}^{n}x_{i}\\log(h_{\\theta}(X))+(1-x_{i})\\log(1-h_{\\theta}(X))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572391ee",
   "metadata": {},
   "source": [
    "### Derivative of the Cost Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f26d4f",
   "metadata": {},
   "source": [
    "Its hard to do by hand -__- wkwkwk"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
